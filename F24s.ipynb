{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c3b7e0",
   "metadata": {},
   "source": [
    "## Generating Data Using PyMultiNest + Expressions\n",
    "\n",
    "the output data is saved as 'F24s-.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DYLD_LIBRARY_PATH=/Users/vahid/libs/MultiNest/lib\n",
    "%env LD_LIBRARY_PATH=/Users/vahid/libs/MultiNest/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e23e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymultinest\n",
    "import pickle\n",
    "from sympy import symbols, lambdify\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 0: Ensure output directory exists\n",
    "# ------------------------------------------------------\n",
    "output_dir = 'mn_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 1: Load symbolic regression models (.pck files)\n",
    "# ------------------------------------------------------\n",
    "m0_sym, m12_sym, A0_sym, tanb_sym = symbols('m0 m12 A0 tanb')\n",
    "input_vars = (m0_sym, m12_sym, A0_sym, tanb_sym)\n",
    "\n",
    "def load_sympy_function(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        expr = pickle.load(f)\n",
    "    return lambdify(input_vars, expr, 'numpy')\n",
    "\n",
    "compute_mh = load_sympy_function('./expressions/mH0-sympy.pck')\n",
    "compute_gminus2 = load_sympy_function('./expressions/g-2-sympy.pck')\n",
    "compute_omega = load_sympy_function('./expressions/Omega-sympy.pck')\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 2: Define priors and parameter space\n",
    "# ------------------------------------------------------\n",
    "param_names = ['m0', 'm12', 'A0', 'tanb']\n",
    "param_bounds = {\n",
    "    'm0': (0, 10000),       # 0 to 10 TeV in GeV\n",
    "    'm12': (0, 10000),      # 0 to 10 TeV in GeV\n",
    "    'A0': (-60000, 60000),  # -60 to 60 TeV in GeV\n",
    "    'tanb': (1.5, 50)       # dimensionless\n",
    "}\n",
    "\n",
    "def prior_transform(cube, ndim, nparams):\n",
    "    for i, key in enumerate(param_names):\n",
    "        low, high = param_bounds[key]\n",
    "        cube[i] = low + cube[i] * (high - low)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 3: Gaussian Log-Likelihood based on benchmark values\n",
    "# ------------------------------------------------------\n",
    "# Benchmark observable values and uncertainties\n",
    "mu_mh, sigma_mh = 125.1, 0.2\n",
    "mu_g2, sigma_g2 = 2.51e-9, 1.0e-9\n",
    "mu_omega, sigma_omega = 0.12, 0.01\n",
    "\n",
    "# samples = []\n",
    "\n",
    "def loglike(cube, ndim, nparams):\n",
    "    m0, m12, A0, tanb = cube[0], cube[1], cube[2], cube[3]\n",
    "\n",
    "    try:\n",
    "        mh = compute_mh(m0, m12, A0, tanb)\n",
    "        g2 = compute_gminus2(m0, m12, A0, tanb)\n",
    "        omega = compute_omega(m0, m12, A0, tanb)\n",
    "        cube[4], cube[5], cube[6] = mh , g2, omega \n",
    "        # Check for invalid results\n",
    "        if not np.all(np.isfinite([mh, g2, omega])):\n",
    "            return -1e90\n",
    "\n",
    "        # Gaussian log-likelihood\n",
    "        chi2 = ((mh - mu_mh) ** 2 / sigma_mh ** 2 +\n",
    "                (g2 - mu_g2) ** 2 / sigma_g2 ** 2 +\n",
    "                (omega - mu_omega) ** 2 / sigma_omega ** 2)\n",
    "\n",
    "        # if chi2 > 1e6:  # reasonable cutoff\n",
    "        #     return -1e90  # discard the sample\n",
    "       \n",
    "        logL = -0.5 * chi2\n",
    "\n",
    "        # samples.append((m0, m12, A0, tanb, mh, g2, omega, logL))\n",
    "\n",
    "        return logL\n",
    "\n",
    "    except Exception:\n",
    "        return -1e90\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 4: Run MultiNest\n",
    "# ------------------------------------------------------\n",
    "pymultinest.run(\n",
    "    loglike,\n",
    "    prior_transform,\n",
    "    n_dims = 4,\n",
    "    n_params= 7,\n",
    "    n_live_points=2000,\n",
    "    evidence_tolerance=0.5,\n",
    "    sampling_efficiency='model',\n",
    "    outputfiles_basename=f'{output_dir}/F24s-',\n",
    "    resume=False,\n",
    "    verbose=True,\n",
    "    max_iter=20000\n",
    ")\n",
    "\n",
    "F24s = np.loadtxt('./mn_output/F24s-.txt')\n",
    "print(f\"\\n✅ Saved {len(F24s)} samples to F24s-.txt with MultiNest weights.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Since we modified the code to save the observables in .txt file too,\n",
    "There's no need for the code below\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 5: Save output to F24-synthetic.txt\n",
    "# ------------------------------------------------------\n",
    "# Read MultiNest posterior samples\n",
    "# posterior_file = f'{output_dir}/.txt'  # MultiNest's posterior file\n",
    "# data = np.loadtxt(posterior_file)  # Columns: weight, logL, m0, m12, A0, tanb\n",
    "# weights = data[:, 0]\n",
    "# logLs = data[:, 1]\n",
    "# params = data[:, 2:6]  # m0, m12, A0, tanb\n",
    "\n",
    "# Compute observables for each sample\n",
    "# observables = []\n",
    "# for p in params:\n",
    "#     m0, m12, A0, tanb = p\n",
    "#     try:\n",
    "#         mh = compute_mh(m0, m12, A0, tanb)\n",
    "#         g2 = compute_gminus2(m0, m12, A0, tanb)\n",
    "#         omega = compute_omega(m0, m12, A0, tanb)\n",
    "#         if not np.all(np.isfinite([mh, g2, omega])):\n",
    "#             mh, g2, omega = np.nan, np.nan, np.nan\n",
    "#     except:\n",
    "#         mh, g2, omega = np.nan, np.nan, np.nan\n",
    "#     observables.append([mh, g2, omega])\n",
    "# observables = np.array(observables)\n",
    "\n",
    "# Combine weights, logL, parameters, and observables\n",
    "# final_data = np.column_stack((weights, logLs, params, observables))\n",
    "# np.savetxt('F24-synthetic.txt', final_data, fmt='%.18e')\n",
    "\n",
    "# print(f\"\\n✅ Saved {len(final_data)} samples to F24-synthetic.txt with MultiNest weights.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6f683",
   "metadata": {},
   "source": [
    "## Triangle plot of the F24s-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42499fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from getdist import MCSamples, plots\n",
    "\n",
    "# Load data\n",
    "samples = np.loadtxt(\"./mn_output/F24s-.txt\")\n",
    "psamples = samples[:, 2:]\n",
    "\n",
    "names = ['m0', 'm12', 'A0', 'tanb', 'mh', 'gminus2', 'omega']\n",
    "axes = ['m0', 'm12', 'A0', 'tanb', 'gminus2', 'omega']\n",
    "labels = [\n",
    "    r'm_0', r'm_{1/2}', r'A_0', r'\\tan\\beta',\n",
    "    r'M_{H^0}', r'\\delta (g-2)_\\mu', r'\\Omega_{\\rm DM}\\, h^2'\n",
    "]\n",
    "\n",
    "# Create MCSamples\n",
    "g_samp = MCSamples(samples=psamples,\n",
    "                   names=names,\n",
    "                   labels=labels,\n",
    "                   name_tag=\"F24\")\n",
    "\n",
    "\n",
    "# Plot\n",
    "g = plots.get_subplot_plotter(width_inch=8)\n",
    "g.triangle_plot([g_samp], axes, plot_3d_with_param=\"mh\", legend_labels=[\"F24s-\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba2573",
   "metadata": {},
   "source": [
    "## Bayesflow (Likelihood-Free approach)\n",
    "\n",
    "Using Bayesflow to generate new posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bayesflow\n",
    "from bayesflow.summary_networks import DeepSet\n",
    "from bayesflow.inference_networks import InvertibleNetwork\n",
    "from bayesflow.amortizers import AmortizedPosterior\n",
    "from bayesflow.trainers import Trainer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from scipy.stats import gaussian_kde\n",
    "import os\n",
    "\n",
    "# Load and preprocess data\n",
    "data = np.loadtxt('./mn_output/F24s-.txt')\n",
    "params = data[:, 2:6]         # 4 independent parameters: m0, m12, A0, tanb\n",
    "observables = data[:, 6:9]    # 3 observables: mh, gminus2, omega\n",
    "\n",
    "# Check for NaNs or infinite values\n",
    "if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
    "    raise ValueError(\"Data contains NaNs or infinite values. Please clean the data.\")\n",
    "\n",
    "# Normalize data\n",
    "def normalize(x):\n",
    "    m, s = x.mean(0), x.std(0)\n",
    "    s = np.where(s == 0, 1e-10, s)  # Avoid division by zero\n",
    "    return (x - m) / s, m, s\n",
    "\n",
    "p_norm, p_mean, p_std = normalize(params)\n",
    "o_norm, o_mean, o_std = normalize(observables)\n",
    "\n",
    "# Subsample for training and posterior sampling (one-fifth of data)\n",
    "np.random.seed(42)\n",
    "idx = np.random.choice(len(data), len(data)//5, replace=False)\n",
    "params_sub, obs_sub = params[idx], observables[idx]\n",
    "p_norm_sub, o_norm_sub = p_norm[idx], o_norm[idx]\n",
    "\n",
    "# Train/val split\n",
    "split = int(0.8 * len(p_norm_sub))\n",
    "p_train, p_val = p_norm_sub[:split], p_norm_sub[split:]\n",
    "o_train, o_val = o_norm_sub[:split], o_norm_sub[split:]\n",
    "\n",
    "# Reshape to (N,1,D)\n",
    "p3_train = p_train[:, None, :]\n",
    "o3_train = o_train[:, None, :]\n",
    "p3_val = p_val[:, None, :]\n",
    "o3_val = o_val[:, None, :]\n",
    "\n",
    "# Validate shapes\n",
    "print(f\"p3_train shape: {p3_train.shape}\")\n",
    "print(f\"o3_train shape: {o3_train.shape}\")\n",
    "if p3_train.shape[0] != o3_train.shape[0]:\n",
    "    raise ValueError(\"Mismatch in number of training samples between p3_train and o3_train.\")\n",
    "if p3_train.shape[0] == 0:\n",
    "    raise ValueError(\"Training data is empty.\")\n",
    "\n",
    "sim_dict = {'prior_draws': p3_train, 'sim_data': o3_train}\n",
    "\n",
    "# Define BayesFlow networks\n",
    "summary_net = DeepSet(\n",
    "    summary_dim=8,\n",
    "    num_dense_s1=2,\n",
    "    num_dense_s2=2,\n",
    "    num_dense_s3=1,\n",
    "    dense_s1_args={\"units\": 64, \"activation\": \"relu\"},\n",
    "    dense_s2_args={\"units\": 64, \"activation\": \"relu\"},\n",
    "    dense_s3_args={\"units\": 64, \"activation\": \"relu\"},\n",
    "    pooling_fun=\"mean\"\n",
    ")\n",
    "\n",
    "inference_net = InvertibleNetwork(\n",
    "    num_params=4,\n",
    "    num_coupling_layers=5,\n",
    "    use_act_norm=True\n",
    ")\n",
    "\n",
    "amortizer = AmortizedPosterior(\n",
    "    summary_net=summary_net,\n",
    "    inference_net=inference_net\n",
    ")\n",
    "\n",
    "# Initialize and train the model with smaller batch size\n",
    "trainer = Trainer(amortizer=amortizer, generative_model=None)\n",
    "trainer.train_offline(sim_dict, epochs=50, batch_size=64,  # Reduced from 128\n",
    "                     callbacks=[EarlyStopping(patience=10), ReduceLROnPlateau()])\n",
    "\n",
    "# Generate posterior samples for the SUBSAMPLED dataset\n",
    "o_sub = o_norm_sub[:, None, :]  # Reshape subsampled observables\n",
    "input_dict = {'summary_conditions': o_sub}\n",
    "post_samples = amortizer.sample(input_dict, n_samples=100)  # Generate 100 samples per observable\n",
    "post_denorm = post_samples * p_std[None, None, :] + p_mean[None, None, :]  # Denormalize\n",
    "\n",
    "# Save\n",
    "np.savetxt(\"BFLF-post.txt\", post_denorm.reshape(-1, 4), header=\"m0 m12 A0 tanb\", fmt=\"%.6f\")\n",
    "\n",
    "print(\"Posterior samples saved to BFLF-post.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a7107",
   "metadata": {},
   "source": [
    "## Bayesflow (Likelihood-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulator\n",
    "def simulator(theta, mu_mh=125.0, sigma_mh=0.5, mu_g2=0.0, sigma_g2=1e-10, mu_omega=0.12, sigma_omega=0.01):\n",
    "    m0, m12, A0, tanb = theta[:, 0], theta[:, 1], theta[:, 2], theta[:, 3]\n",
    "    try:\n",
    "        mh = np.array([compute_mh(m0[i], m12[i], A0[i], tanb[i]) for i in range(len(m0))])\n",
    "        g2 = np.array([compute_gminus2(m0[i], m12[i], A0[i], tanb[i]) for i in range(len(m0))])\n",
    "        omega = np.array([compute_omega(m0[i], m12[i], A0[i], tanb[i]) for i in range(len(m0))])\n",
    "\n",
    "        mh += np.random.normal(0, sigma_mh, size=mh.shape)\n",
    "        g2 += np.random.normal(0, sigma_g2, size=g2.shape)\n",
    "        omega += np.random.normal(0, sigma_omega, size=omega.shape)\n",
    "\n",
    "        if not np.all(np.isfinite([mh, g2, omega])):\n",
    "            mh, g2, omega = np.zeros_like(mh), np.zeros_like(g2), np.zeros_like(omega)\n",
    "\n",
    "        sim2d = np.stack([mh, g2, omega], axis=-1)\n",
    "        return sim2d\n",
    "    except Exception:\n",
    "        return np.zeros((len(m0), 3))\n",
    "\n",
    "# Prior\n",
    "def prior(batch_size):\n",
    "    m0 = np.random.uniform(0, 10000, batch_size)\n",
    "    m12 = np.random.uniform(0, 10000, batch_size)\n",
    "    A0 = np.random.uniform(-60000, 60000, batch_size)\n",
    "    tanb = np.random.uniform(1.5, 50, batch_size)\n",
    "    return np.stack([m0, m12, A0, tanb], axis=-1)\n",
    "\n",
    "\n",
    "# Offline data dictionary\n",
    "sim_dict = {\n",
    "    'prior_draws': p3_train,\n",
    "    'sim_data': s3_train,\n",
    "}\n",
    "\n",
    "# Define networks\n",
    "summary_net = DeepSet(\n",
    "    summary_dim=8,\n",
    "    num_dense_s1=2, num_dense_s2=2, num_dense_s3=2,\n",
    "    dense_s1_args={\"units\": 64, \"activation\": \"relu\"},\n",
    "    dense_s2_args={\"units\": 64, \"activation\": \"relu\"},\n",
    "    dense_s3_args={\"units\": 64, \"activation\": \"relu\"},\n",
    "    pooling_fun=\"mean\"\n",
    ")\n",
    "\n",
    "inference_net = InvertibleNetwork(\n",
    "    num_params=4,\n",
    "    num_coupling_layers=5,\n",
    "    use_act_norm=True\n",
    ")\n",
    "generative_model = GenerativeModel(prior, simulator, simulator_is_batched=True, prior_is_batched=True)\n",
    "amortizer = AmortizedPosterior(summary_net=summary_net, inference_net=inference_net)\n",
    "\n",
    "trainer = Trainer(amortizer=amortizer, generative_model=generative_model)\n",
    "\n",
    "# Train\n",
    "trainer.train_offline(sim_dict, epochs=39, batch_size=64,\n",
    "                      callbacks=[EarlyStopping(patience=10), ReduceLROnPlateau()])\n",
    "\n",
    "# Generate posterior samples\n",
    "post_samples = amortizer.sample(input_dict, n_samples=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874eb785",
   "metadata": {},
   "source": [
    "## Plotting Bayesflow vs MultiNest\n",
    "Since BayesFlow is designed to infer the posterior distribution of the parameters <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>m</mi><mn>0</mn><mo separator=\"true\">,</mo><mi>m</mi><mn>12</mn><mo separator=\"true\">,</mo><mi>A</mi><mn>0</mn><mo separator=\"true\">,</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>b</mi><mi mathvariant=\"normal\">∣</mi><mi>m</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>g</mi><mi>m</mi><mi>i</mi><mi>n</mi><mi>u</mi><mi>s</mi><mn>2</mn><mo separator=\"true\">,</mo><mi>o</mi><mi>m</mi><mi>e</mi><mi>g</mi><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\"> p(m0, m12, A0, tanb | mh, gminus2, omega) </annotation></semantics></math>, using the observables as conditioning data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ea943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from getdist import MCSamples, plots\n",
    "\n",
    "# Load BayesFlow posteriors\n",
    "bf_pst = np.loadtxt('BFLF-post.txt', skiprows=1) \n",
    "\n",
    "# Load MultiNest data\n",
    "data = np.loadtxt('./mn_output/F24s-.txt')\n",
    "params = data[:, 2:6]  \n",
    "weights = data[:, 0]  \n",
    "\n",
    "# Define parameter names and labels\n",
    "names = ['m0', 'm12', 'A0', 'tanb']\n",
    "labels = [r'm_0', r'm_{12}', r'A_0', r'\\tan\\beta']\n",
    "\n",
    "# Create MCSamples objects\n",
    "bflf_samples = MCSamples(samples=bf_pst, names=names, labels=labels, \n",
    "                       label='BayesFlow Posteriors')\n",
    "orig_samples = MCSamples(samples=params, weights=weights, names=names, \n",
    "                         labels=labels, label='MultiNest Posteriors')\n",
    "\n",
    "# Plot triangle plot\n",
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot([bflf_samples, orig_samples], ['m0', 'm12', 'A0', 'tanb'], \n",
    "                filled=True, colors=['blue', 'red'], \n",
    "                legend_labels=['BayesFlow-LF', 'MultiNest'])\n",
    "\n",
    "print(bflf_samples.getTable().tableTex())\n",
    "print(orig_samples.getTable().tablePNG())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bflf_samples.getTable().tableTex())\n",
    "\n",
    "# print(orig_samples.getTable().tableTex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c403737",
   "metadata": {},
   "source": [
    "***BayesFlow-LikelihoodFree***\n",
    "| Parameter        | 95% limits          |\n",
    "|------------------|----------------------|\n",
    "| $m_0$            | $3698^{+5000}_{-4000}$      |\n",
    "| $m_{12}$         | $6308^{+3000}_{-4000}$      |\n",
    "| $A_0$            | $32600^{+30000}_{-30000}$   |\n",
    "| $\\tan\\beta$      | $29^{+20}_{-20}$            |\n",
    "\n",
    "***MultiNest***\n",
    "\n",
    "| Parameter        | 95% limits          |\n",
    "|------------------|----------------------|\n",
    "| $m_0$            | $3542^{+6000}_{-3000}$      |\n",
    "| $m_{12}$         | $6189^{+3000}_{-3000}$      |\n",
    "| $A_0$            | $32000^{+30000}_{-30000}$   |\n",
    "| $\\tan\\beta$      | $28^{+20}_{-20}$            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c4b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each parameter 1D marginalized posterior with normalized=True\n",
    "for param in names:\n",
    "    # Create a GetDist plotter instance\n",
    "    g = plots.get_single_plotter(width_inch=4)\n",
    "    g.plot_1d([bflf_samples, orig_samples], param, marker=0, title_limit=1, normalized=True, colors=['blue', 'red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749c561",
   "metadata": {},
   "source": [
    "## Harmonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4174f",
   "metadata": {},
   "source": [
    "### RealNVPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import harmonic as hm\n",
    "\n",
    "# Section 1: Load MultiNest Output\n",
    "# -------------------------------\n",
    "# Load the data file containing posterior weights, log-likelihoods, and parameters\n",
    "# Column 1: posterior weights\n",
    "# Column 2: log-likelihoods\n",
    "# Columns 3-9: parameters ['m0', 'm12', 'A0', 'tanb', 'mh', 'gminus2', 'omega']\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the data from the specified file and check for numerical issues.\"\"\"\n",
    "    data = np.loadtxt(file_path)\n",
    "    weights = data[:, 0]  # Posterior weights\n",
    "    log_likelihoods = data[:, 1]  # Log-likelihoods (already in logL form)\n",
    "    samples = data[:, 2:]  # Parameters (7 dimensions)\n",
    "    \n",
    "    # Check for inf/nan in log-likelihoods and samples\n",
    "    if np.any(np.isinf(log_likelihoods)) or np.any(np.isnan(log_likelihoods)):\n",
    "        print(\"Warning: log_likelihoods contain inf/nan values\")\n",
    "    if np.any(np.isinf(samples)) or np.any(np.isnan(samples)):\n",
    "        print(\"Warning: samples contain inf/nan values\")\n",
    "    \n",
    "    # Clip log-likelihoods to prevent extreme values\n",
    "    log_likelihoods = np.clip(log_likelihoods, -1e10, 1e10)\n",
    "    \n",
    "    # Print data statistics for debugging\n",
    "    print(f\"Number of samples: {len(samples)}\")\n",
    "    print(f\"Log-likelihood range: {log_likelihoods.min():.4f} to {log_likelihoods.max():.4f}\")\n",
    "    \n",
    "    return weights, log_likelihoods, samples\n",
    "\n",
    "file_path = './mn_output/F24s-.txt'\n",
    "weights, log_likelihoods, samples = load_data(file_path)\n",
    "\n",
    "# Section 2: Prepare Data for Harmonic\n",
    "# -----------------------------------\n",
    "# Reshape samples and log-likelihoods into pseudo-chains for harmonic\n",
    "n_samples_total = samples.shape[0]  # Total number of samples\n",
    "n_dim = samples.shape[1]  # 7 parameters\n",
    "n_chains = 4  # Increased to 4 chains for better error estimation\n",
    "n_samples_per_chain = n_samples_total // n_chains  # Integer division for equal splits\n",
    "\n",
    "# Check if enough samples per chain\n",
    "if n_samples_per_chain < 200:\n",
    "    print(f\"Warning: Only {n_samples_per_chain} samples per chain. Consider increasing samples.\")\n",
    "\n",
    "# Trim samples to ensure they fit n_chains * n_samples_per_chain\n",
    "samples = samples[:n_chains * n_samples_per_chain]\n",
    "log_weights = np.log(weights[:n_chains * n_samples_per_chain])\n",
    "log_weights -= np.max(log_weights)\n",
    "\n",
    "# Reshape to (n_chains, n_samples_per_chain, n_dim)\n",
    "samples = samples.reshape(n_chains, n_samples_per_chain, n_dim)\n",
    "log_weights = log_weights.reshape(n_chains, n_samples_per_chain)\n",
    "\n",
    "# Initialize Chains object\n",
    "chains = hm.Chains(ndim=n_dim)\n",
    "\n",
    "# Add samples and log-likelihoods to Chains\n",
    "chains.add_chains_3d(samples, ln_posterior=log_weights)\n",
    "\n",
    "# Split data into training and inference sets with more inference samples\n",
    "chains_train, chains_infer = hm.utils.split_data(chains, training_proportion=0.3)  # Reduced to 30% training\n",
    "\n",
    "# Section 3: Train Normalizing Flow Model\n",
    "# --------------------------------------\n",
    "# Initialize and train a RealNVPModel for evidence estimation\n",
    "model = hm.model.RealNVPModel(\n",
    "    ndim_in=n_dim,\n",
    "    n_scaled_layers=1,          # Further reduced for stability\n",
    "    n_unscaled_layers=2,        # Further reduced for stability\n",
    "    learning_rate=0.001,        # Kept for convergence\n",
    "    standardize=True,           # Standardize data\n",
    "    temperature=1.0,            # Increased to 1.0 for smoother distribution\n",
    ")\n",
    "\n",
    "# Train model and print loss for debugging\n",
    "model.fit(chains_train.samples)\n",
    "\n",
    "# Section 4: Compute Evidence\n",
    "# ---------------------------\n",
    "# Use harmonic to compute the evidence and its error with clipping\n",
    "ev = hm.Evidence(chains_infer.nchains, model)\n",
    "ev.add_chains(chains_infer)\n",
    "ln_inv_evidence = ev.ln_evidence_inv\n",
    "\n",
    "# Clip ln_inv_evidence to prevent inf\n",
    "if np.isinf(ln_inv_evidence) or np.isnan(ln_inv_evidence):\n",
    "    print(\"Warning: ln_inv_evidence is inf or nan, clipping to large value\")\n",
    "    ln_inv_evidence = np.clip(ln_inv_evidence, -1e10, 1e10)\n",
    "\n",
    "# Compute errors and handle inf/nan\n",
    "err_ln_inv_evidence = ev.compute_ln_inv_evidence_errors()\n",
    "print(f\"ln_inv_evidence: {ln_inv_evidence:.4f}\")\n",
    "print(f\"err_ln_inv_evidence: {err_ln_inv_evidence}\")\n",
    "\n",
    "# Convert to log-evidence (ln(Z) = -ln(1/Z)) and extract error\n",
    "harmonic_logZ = -ln_inv_evidence\n",
    "err_ln_evidence = max(abs(err_ln_inv_evidence[0]), abs(err_ln_inv_evidence[1]))\n",
    "\n",
    "# Section 5: Display Results\n",
    "# -------------------------\n",
    "# Print the computed log-evidence and its error in the requested format\n",
    "print(f\"Harmonic Log Evidence (RealNVPModel): {harmonic_logZ:.4f} +/- {err_ln_evidence:.4f}\")\n",
    "\n",
    "\n",
    "# Parse /data-synthetic/stats.dat to get Global Log-Evidence from synthetic\n",
    "with open('./mn_output/F24s-stats.dat', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if 'Nested Sampling Global Log-Evidence' in line:\n",
    "            # Extract the number after ':'\n",
    "            mn_logZ = float(line.split(':')[1].split('+/-')[0].strip())\n",
    "            err_mn_logZ = float(line.split(':')[1].split('+/-')[1].strip())\n",
    "            break\n",
    "\n",
    "print(f\"PyMultiNest log evidence (ln(Z)): {mn_logZ:.4f} +/- {err_mn_logZ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ca455",
   "metadata": {},
   "source": [
    "### RQSplineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b82d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import harmonic as hm\n",
    "\n",
    "# Section 1: Load MultiNest Output\n",
    "# -------------------------------\n",
    "# Load the data file containing posterior weights, log-likelihoods, and parameters\n",
    "# Column 1: posterior weights\n",
    "# Column 2: log-likelihoods\n",
    "# Columns 3-9: parameters ['m0', 'm12', 'A0', 'tanb', 'mh', 'gminus2', 'omega']\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the data from the specified file and check for numerical issues.\"\"\"\n",
    "    data = np.loadtxt(file_path)\n",
    "    weights = data[:, 0]  # Posterior weights\n",
    "    log_likelihoods = data[:, 1]  # Log-likelihoods (already in logL form)\n",
    "    samples = data[:, 2:]  # Parameters (7 dimensions)\n",
    "    \n",
    "    # Check for inf/nan in log-likelihoods and samples\n",
    "    if np.any(np.isinf(log_likelihoods)) or np.any(np.isnan(log_likelihoods)):\n",
    "        print(\"Warning: log_likelihoods contain inf/nan values\")\n",
    "    if np.any(np.isinf(samples)) or np.any(np.isnan(samples)):\n",
    "        print(\"Warning: samples contain inf/nan values\")\n",
    "    \n",
    "    # Clip log-likelihoods to prevent extreme values\n",
    "    log_likelihoods = np.clip(log_likelihoods, -1e10, 1e10)\n",
    "    \n",
    "    # Print data statistics for debugging\n",
    "    print(f\"Number of samples: {len(samples)}\")\n",
    "    print(f\"Log-likelihood range: {log_likelihoods.min():.4f} to {log_likelihoods.max():.4f}\")\n",
    "    \n",
    "    return weights, log_likelihoods, samples\n",
    "\n",
    "file_path = './mn_output/F24s-.txt'\n",
    "weights, log_likelihoods, samples = load_data(file_path)\n",
    "\n",
    "# Section 2: Prepare Data for Harmonic\n",
    "# -----------------------------------\n",
    "# Reshape samples and log-likelihoods into pseudo-chains for harmonic\n",
    "n_samples_total = samples.shape[0]  # Total number of samples\n",
    "n_dim = samples.shape[1]  # 7 parameters\n",
    "n_chains = 8  # Increased to 8 to allow training_proportion=0.25\n",
    "n_samples_per_chain = n_samples_total // n_chains  # Integer division for equal splits\n",
    "\n",
    "# Check if enough samples per chain\n",
    "if n_samples_per_chain < 200:\n",
    "    print(f\"Warning: Only {n_samples_per_chain} samples per chain. Consider increasing samples.\")\n",
    "\n",
    "# Trim samples to ensure they fit n_chains * n_samples_per_chain\n",
    "samples = samples[:n_chains * n_samples_per_chain]\n",
    "log_weights = np.log(weights[:n_chains * n_samples_per_chain])\n",
    "log_weights -= np.max(log_weights)\n",
    "\n",
    "# Reshape to (n_chains, n_samples_per_chain, n_dim)\n",
    "samples = samples.reshape(n_chains, n_samples_per_chain, n_dim)\n",
    "log_weights = log_weights.reshape(n_chains, n_samples_per_chain)\n",
    "\n",
    "\n",
    "# Initialize Chains object\n",
    "chains = hm.Chains(ndim=n_dim)\n",
    "\n",
    "# Add samples and log of posterior weights to Chains\n",
    "chains.add_chains_3d(samples, ln_posterior=log_weights)\n",
    "\n",
    "# Split data into training and inference sets with more inference samples\n",
    "chains_train, chains_infer = hm.utils.split_data(chains, training_proportion=0.25)  # 25% training\n",
    "\n",
    "# Print inference sample count for debugging\n",
    "print(f\"Inference samples: {chains_infer.samples.shape[0] * chains_infer.samples.shape[1]}\")\n",
    "\n",
    "# Section 3: Train Normalizing Flow Model\n",
    "# --------------------------------------\n",
    "# Initialize and train an RQSplineModel for evidence estimation\n",
    "model = hm.model.RQSplineModel(\n",
    "    ndim_in=n_dim,\n",
    "    n_layers=6,                 # Reduced from default 8 for stability\n",
    "    n_bins=8,                   # Default, kept for flexibility\n",
    "    hidden_size=[128, 128],       # Default, kept for capacity\n",
    "    spline_range=(-10, 10),   # Narrowed to constrain transformations\n",
    "    standardize=True,           # Standardize data\n",
    "    learning_rate=0.001,        # Kept for stable convergence\n",
    "    momentum=0.9,               # Default for Adam optimizer\n",
    "    temperature=0.5             # Kept to tighten distribution\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(chains_train.samples)\n",
    "\n",
    "# Section 4: Compute Evidence\n",
    "# ---------------------------\n",
    "# Use harmonic to compute the evidence and its error\n",
    "ev = hm.Evidence(chains_infer.nchains, model)\n",
    "ev.add_chains(chains_infer)\n",
    "ln_inv_evidence = ev.ln_evidence_inv\n",
    "\n",
    "# Clip ln_inv_evidence to prevent inf\n",
    "if np.isinf(ln_inv_evidence) or np.isnan(ln_inv_evidence):\n",
    "    print(\"Warning: ln_inv_evidence is inf or nan, clipping to large value\")\n",
    "    ln_inv_evidence = np.clip(ln_inv_evidence, -1e10, 1e10)\n",
    "\n",
    "# Compute errors and handle inf/nan\n",
    "err_ln_inv_evidence = ev.compute_ln_inv_evidence_errors()\n",
    "print(f\"ln_inv_evidence: {ln_inv_evidence:.4f}\")\n",
    "print(f\"err_ln_inv_evidence: {err_ln_inv_evidence}\")\n",
    "\n",
    "# Convert to log-evidence (ln(Z) = -ln(1/Z)) and extract error\n",
    "harmonic_logZ = -ln_inv_evidence\n",
    "err_ln_evidence = max(abs(err_ln_inv_evidence[0]), abs(err_ln_inv_evidence[1]))\n",
    "\n",
    "# Section 5: Display Results\n",
    "# -------------------------\n",
    "# Print the computed log-evidence and its error in the requested format\n",
    "print(f\"Harmonic Log Evidence (RQSplineModel): {harmonic_logZ:.4f} +/- {err_ln_evidence:.4f}\")\n",
    "\n",
    "print(f\"PyMultiNest log evidence (ln(Z)): {mn_logZ:.4f} +/- {err_mn_logZ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902edab",
   "metadata": {},
   "source": [
    "## Random Scan over Parameter Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5b8c8",
   "metadata": {},
   "source": [
    "### P_r = 1 , LogL = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sympy import symbols, lambdify\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 0: Ensure output directory exists\n",
    "# ------------------------------------------------------\n",
    "output_dir = 'randomscan_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'F24s_random.txt')\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 1: Load symbolic regression models\n",
    "# ------------------------------------------------------\n",
    "m0_sym, m12_sym, A0_sym, tanb_sym = symbols('m0 m12 A0 tanb')\n",
    "input_vars = (m0_sym, m12_sym, A0_sym, tanb_sym)\n",
    "\n",
    "def load_sympy_function(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        expr = pickle.load(f)\n",
    "    return lambdify(input_vars, expr, 'numpy')\n",
    "\n",
    "compute_mh = load_sympy_function('./expressions/mH0-sympy.pck')\n",
    "compute_g2 = load_sympy_function('./expressions/g-2-sympy.pck')\n",
    "compute_omega = load_sympy_function('./expressions/Omega-sympy.pck')\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 2: Parameter bounds\n",
    "# ------------------------------------------------------\n",
    "param_bounds = {\n",
    "    'm0': (0, 10000),\n",
    "    'm12': (0, 10000),\n",
    "    'A0': (-60000, 60000),\n",
    "    'tanb': (1.5, 50)\n",
    "}\n",
    "param_names = list(param_bounds.keys())\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 3: Perform random scan\n",
    "# ------------------------------------------------------\n",
    "n_samples = 1000000\n",
    "data = []\n",
    "\n",
    "for _ in tqdm(range(n_samples), desc=\"Random scanning\"):\n",
    "    # Sample random parameters from uniform prior\n",
    "    m0 = np.random.uniform(*param_bounds['m0'])\n",
    "    m12 = np.random.uniform(*param_bounds['m12'])\n",
    "    A0 = np.random.uniform(*param_bounds['A0'])\n",
    "    tanb = np.random.uniform(*param_bounds['tanb'])\n",
    "\n",
    "    try:\n",
    "        # Compute observables\n",
    "        mh = compute_mh(m0, m12, A0, tanb)\n",
    "        g2 = compute_g2(m0, m12, A0, tanb)\n",
    "        omega = compute_omega(m0, m12, A0, tanb)\n",
    "\n",
    "        if not np.all(np.isfinite([mh, g2, omega])):\n",
    "            continue  # skip invalid samples\n",
    "\n",
    "        # Format: posterior_weight | log_likelihood | m0 m12 A0 tanb | mh g2 omega\n",
    "        row = [1.0, 0.0, m0, m12, A0, tanb, mh, g2, omega]\n",
    "        data.append(row)\n",
    "\n",
    "    except Exception:\n",
    "        continue  # skip any errors silently\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 4: Save output file\n",
    "# ------------------------------------------------------\n",
    "data = np.array(data)\n",
    "np.savetxt(output_file, data, fmt='%.6e')\n",
    "print(f\"\\n✅ Saved {len(data)} samples to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a109578",
   "metadata": {},
   "source": [
    "### P_r = 1, LogL != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5185224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sympy import symbols, lambdify\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 0: Ensure output directory exists\n",
    "# ------------------------------------------------------\n",
    "output_dir = 'randomscan_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'F24s_random_withlogL.txt')\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 1: Load symbolic regression models\n",
    "# ------------------------------------------------------\n",
    "m0_sym, m12_sym, A0_sym, tanb_sym = symbols('m0 m12 A0 tanb')\n",
    "input_vars = (m0_sym, m12_sym, A0_sym, tanb_sym)\n",
    "\n",
    "def load_sympy_function(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        expr = pickle.load(f)\n",
    "    return lambdify(input_vars, expr, 'numpy')\n",
    "\n",
    "compute_mh = load_sympy_function('./expressions/mH0-sympy.pck')\n",
    "compute_g2 = load_sympy_function('./expressions/g-2-sympy.pck')\n",
    "compute_omega = load_sympy_function('./expressions/Omega-sympy.pck')\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 2: Parameter bounds\n",
    "# ------------------------------------------------------\n",
    "param_bounds = {\n",
    "    'm0': (0, 10000),\n",
    "    'm12': (0, 10000),\n",
    "    'A0': (-60000, 60000),\n",
    "    'tanb': (1.5, 50)\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 3: Observational values for likelihood\n",
    "# ------------------------------------------------------\n",
    "# These are placeholders; replace with correct experimental values and errors.\n",
    "mh_obs, mh_err = 125.1, 2.0\n",
    "g2_obs, g2_err = 2.5e-9, 0.5e-9\n",
    "omega_obs, omega_err = 0.12, 0.01\n",
    "\n",
    "def log_likelihood(mh, g2, omega):\n",
    "    chi2_mh = ((mh - mh_obs) / mh_err) ** 2\n",
    "    chi2_g2 = ((g2 - g2_obs) / g2_err) ** 2\n",
    "    chi2_omega = ((omega - omega_obs) / omega_err) ** 2\n",
    "    chi2_total = chi2_mh + chi2_g2 + chi2_omega\n",
    "    return -0.5 * chi2_total\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 4: Perform random scan\n",
    "# ------------------------------------------------------\n",
    "n_samples = 1000000\n",
    "data = []\n",
    "\n",
    "for _ in tqdm(range(n_samples), desc=\"Random scanning\"):\n",
    "    m0 = np.random.uniform(*param_bounds['m0'])\n",
    "    m12 = np.random.uniform(*param_bounds['m12'])\n",
    "    A0 = np.random.uniform(*param_bounds['A0'])\n",
    "    tanb = np.random.uniform(*param_bounds['tanb'])\n",
    "\n",
    "    try:\n",
    "        mh = compute_mh(m0, m12, A0, tanb)\n",
    "        g2 = compute_g2(m0, m12, A0, tanb)\n",
    "        omega = compute_omega(m0, m12, A0, tanb)\n",
    "\n",
    "        if not np.all(np.isfinite([mh, g2, omega])):\n",
    "            continue\n",
    "\n",
    "        logL = log_likelihood(mh, g2, omega)\n",
    "        row = [1.0, logL, m0, m12, A0, tanb, mh, g2, omega]\n",
    "        data.append(row)\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 5: Save output file\n",
    "# ------------------------------------------------------\n",
    "data = np.array(data)\n",
    "np.savetxt(output_file, data, fmt='%.6e')\n",
    "print(f\"\\n✅ Saved {len(data)} samples with log-likelihoods to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e905b",
   "metadata": {},
   "source": [
    "## Harmonic Evidence on Random Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5809c7",
   "metadata": {},
   "source": [
    "### P_r = 1, LogL = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178746d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import harmonic as hm\n",
    "\n",
    "# Section 1: Load Random Scan Output\n",
    "# -------------------------------\n",
    "# Load the data file containing posterior weights, log-likelihoods, and parameters\n",
    "# Column 1: posterior weights\n",
    "# Column 2: log-likelihoods\n",
    "# Columns 3-9: parameters ['m0', 'm12', 'A0', 'tanb', 'mh', 'gminus2', 'omega']\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the data from the specified file and check for numerical issues.\"\"\"\n",
    "    data = np.loadtxt(file_path)\n",
    "    weights = data[:, 0]  # Posterior weights\n",
    "    log_likelihoods = data[:, 1]  # Log-likelihoods (already in logL form)\n",
    "    samples = data[:, 2:]  # Parameters (7 dimensions)\n",
    "    \n",
    "    # Check for inf/nan in log-likelihoods and samples\n",
    "    if np.any(np.isinf(log_likelihoods)) or np.any(np.isnan(log_likelihoods)):\n",
    "        print(\"Warning: log_likelihoods contain inf/nan values\")\n",
    "    if np.any(np.isinf(samples)) or np.any(np.isnan(samples)):\n",
    "        print(\"Warning: samples contain inf/nan values\")\n",
    "    \n",
    "    # Clip log-likelihoods to prevent extreme values\n",
    "    log_likelihoods = np.clip(log_likelihoods, -1e10, 1e10)\n",
    "    \n",
    "    # Print data statistics for debugging\n",
    "    print(f\"Number of samples: {len(samples)}\")\n",
    "    print(f\"Log-likelihood range: {log_likelihoods.min():.4f} to {log_likelihoods.max():.4f}\")\n",
    "    \n",
    "    return weights, log_likelihoods, samples\n",
    "\n",
    "file_path = './randomscan_output/F24s_random.txt'\n",
    "weights, log_likelihoods, samples = load_data(file_path)\n",
    "\n",
    "# Reshape to (n_chains, n_samples_per_chain, n_dim)\n",
    "n_samples_total = samples.shape[0]\n",
    "n_chains = 4\n",
    "n_samples_per_chain = n_samples_total // n_chains\n",
    "\n",
    "# Trim samples to ensure they fit n_chains * n_samples_per_chain\n",
    "samples = samples[:n_chains * n_samples_per_chain]\n",
    "log_weights = np.log(weights[:n_chains * n_samples_per_chain])\n",
    "log_weights -= np.max(log_weights)\n",
    "\n",
    "# Reshape to (n_chains, n_samples_per_chain, n_dim)\n",
    "samples = samples.reshape(n_chains, n_samples_per_chain, n_dim)\n",
    "log_weights = log_weights.reshape(n_chains, n_samples_per_chain)\n",
    "\n",
    "chains = hm.Chains(ndim=n_dim)\n",
    "chains.add_chains_3d(samples, ln_posterior=log_weights)\n",
    "\n",
    "chains_train, chains_infer = hm.utils.split_data(chains, training_proportion=0.7)\n",
    "\n",
    "model = hm.model.RealNVPModel(\n",
    "    ndim_in=n_dim,\n",
    "    n_scaled_layers=4,\n",
    "    n_unscaled_layers=8,\n",
    "    learning_rate=0.001,\n",
    "    standardize=True,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "model.fit(chains_train.samples)\n",
    "\n",
    "ev = hm.Evidence(chains_infer.nchains, model)\n",
    "ev.add_chains(chains_infer)\n",
    "ln_inv_evidence = ev.ln_evidence_inv\n",
    "\n",
    "# Clip ln_inv_evidence to prevent inf\n",
    "if np.isinf(ln_inv_evidence) or np.isnan(ln_inv_evidence):\n",
    "    print(\"Warning: ln_inv_evidence is inf or nan, clipping to large value\")\n",
    "    ln_inv_evidence = np.clip(ln_inv_evidence, -1e10, 1e10)\n",
    "\n",
    "# Compute errors and handle inf/nan\n",
    "err_ln_inv_evidence = ev.compute_ln_inv_evidence_errors()\n",
    "print(f\"ln_inv_evidence: {ln_inv_evidence:.4f}\")\n",
    "print(f\"err_ln_inv_evidence: {err_ln_inv_evidence}\")\n",
    "\n",
    "# Convert to log-evidence (ln(Z) = -ln(1/Z)) and extract error\n",
    "harmonic_logZ = -ln_inv_evidence\n",
    "err_ln_evidence = max(abs(err_ln_inv_evidence[0]), abs(err_ln_inv_evidence[1]))\n",
    "\n",
    "# Print the computed log-evidence and its error in the requested format\n",
    "print(f\"Harmonic Log Evidence (RealNVPModel): {harmonic_logZ:.4f} +/- {err_ln_evidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07235465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hm.model.RQSplineModel(\n",
    "    ndim_in=n_dim,\n",
    "    n_layers=4,                 # Reduced from default 8 for stability\n",
    "    n_bins=8,                   # Default, kept for flexibility\n",
    "    hidden_size=[64, 64],       # Default, kept for capacity\n",
    "    spline_range=(-5.0, 5.0),   # Narrowed to constrain transformations\n",
    "    standardize=True,           # Standardize data\n",
    "    learning_rate=0.001,        # Kept for stable convergence\n",
    "    momentum=0.9,               # Default for Adam optimizer\n",
    "    temperature=0.5             # Kept to tighten distribution\n",
    ")\n",
    "\n",
    "model.fit(chains_train.samples)\n",
    "\n",
    "ev = hm.Evidence(chains_infer.nchains, model)\n",
    "ev.add_chains(chains_infer)\n",
    "ln_inv_evidence = ev.ln_evidence_inv\n",
    "\n",
    "# Clip ln_inv_evidence to prevent inf\n",
    "if np.isinf(ln_inv_evidence) or np.isnan(ln_inv_evidence):\n",
    "    print(\"Warning: ln_inv_evidence is inf or nan, clipping to large value\")\n",
    "    ln_inv_evidence = np.clip(ln_inv_evidence, -1e10, 1e10)\n",
    "\n",
    "# Compute errors and handle inf/nan\n",
    "err_ln_inv_evidence = ev.compute_ln_inv_evidence_errors()\n",
    "print(f\"ln_inv_evidence: {ln_inv_evidence:.4f}\")\n",
    "print(f\"err_ln_inv_evidence: {err_ln_inv_evidence}\")\n",
    "\n",
    "# Convert to log-evidence (ln(Z) = -ln(1/Z)) and extract error\n",
    "harmonic_logZ = -ln_inv_evidence\n",
    "err_ln_evidence = max(abs(err_ln_inv_evidence[0]), abs(err_ln_inv_evidence[1]))\n",
    "\n",
    "# Print the computed log-evidence and its error in the requested format\n",
    "print(f\"Harmonic Log Evidence (RQSplineModel): {harmonic_logZ:.4f} +/- {err_ln_evidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53965c59",
   "metadata": {},
   "source": [
    "### P_r = 1, LogL != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99757696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import harmonic as hm\n",
    "\n",
    "# Section 1: Load Random Scan Output\n",
    "# -------------------------------\n",
    "# Load the data file containing posterior weights, log-likelihoods, and parameters\n",
    "# Column 1: posterior weights\n",
    "# Column 2: log-likelihoods\n",
    "# Columns 3-9: parameters ['m0', 'm12', 'A0', 'tanb', 'mh', 'gminus2', 'omega']\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the data from the specified file and check for numerical issues.\"\"\"\n",
    "    data = np.loadtxt(file_path)\n",
    "    weights = data[:, 0]  # Posterior weights\n",
    "    log_likelihoods = data[:, 1]  # Log-likelihoods (already in logL form)\n",
    "    samples = data[:, 2:]  # Parameters (7 dimensions)\n",
    "    \n",
    "    # Check for inf/nan in log-likelihoods and samples\n",
    "    if np.any(np.isinf(log_likelihoods)) or np.any(np.isnan(log_likelihoods)):\n",
    "        print(\"Warning: log_likelihoods contain inf/nan values\")\n",
    "    if np.any(np.isinf(samples)) or np.any(np.isnan(samples)):\n",
    "        print(\"Warning: samples contain inf/nan values\")\n",
    "    \n",
    "    # Clip log-likelihoods to prevent extreme values\n",
    "    log_likelihoods = np.clip(log_likelihoods, -1e10, 1e10)\n",
    "    \n",
    "    # Print data statistics for debugging\n",
    "    print(f\"Number of samples: {len(samples)}\")\n",
    "    print(f\"Log-likelihood range: {log_likelihoods.min():.4f} to {log_likelihoods.max():.4f}\")\n",
    "    \n",
    "    return weights, log_likelihoods, samples\n",
    "\n",
    "file_path = './randomscan_output/F24s_random_withlogL.txt'\n",
    "weights, log_likelihoods, samples = load_data(file_path)\n",
    "\n",
    "# Normalize and threshold log-likelihoods\n",
    "log_likelihoods -= np.max(log_likelihoods)  # Shift max to 0\n",
    "logL_threshold = -25  # Cutoff to reduce dynamic range\n",
    "valid = log_likelihoods > logL_threshold\n",
    "samples = samples[valid]\n",
    "log_likelihoods = log_likelihoods[valid]\n",
    "\n",
    "# Reshape to (n_chains, n_samples_per_chain, n_dim)\n",
    "n_samples_total = samples.shape[0]\n",
    "n_chains = 4\n",
    "n_samples_per_chain = n_samples_total // n_chains\n",
    "\n",
    "samples = samples[:n_chains * n_samples_per_chain]\n",
    "log_likelihoods = log_likelihoods[:n_chains * n_samples_per_chain]\n",
    "\n",
    "samples = samples.reshape(n_chains, n_samples_per_chain, n_dim)\n",
    "log_likelihoods = log_likelihoods.reshape(n_chains, n_samples_per_chain)\n",
    "\n",
    "chains = hm.Chains(ndim=n_dim)\n",
    "chains.add_chains_3d(samples, ln_posterior=log_likelihoods)\n",
    "\n",
    "chains_train, chains_infer = hm.utils.split_data(chains, training_proportion=0.7)\n",
    "\n",
    "model = hm.model.RealNVPModel(\n",
    "    ndim_in=n_dim,\n",
    "    n_scaled_layers=4,\n",
    "    n_unscaled_layers=8,\n",
    "    learning_rate=0.001,\n",
    "    standardize=True,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "model.fit(chains_train.samples)\n",
    "\n",
    "ev = hm.Evidence(chains_infer.nchains, model)\n",
    "ev.add_chains(chains_infer)\n",
    "ln_inv_evidence = ev.ln_evidence_inv\n",
    "\n",
    "# Clip ln_inv_evidence to prevent inf\n",
    "if np.isinf(ln_inv_evidence) or np.isnan(ln_inv_evidence):\n",
    "    print(\"Warning: ln_inv_evidence is inf or nan, clipping to large value\")\n",
    "    ln_inv_evidence = np.clip(ln_inv_evidence, -1e10, 1e10)\n",
    "\n",
    "# Compute errors and handle inf/nan\n",
    "err_ln_inv_evidence = ev.compute_ln_inv_evidence_errors()\n",
    "print(f\"ln_inv_evidence: {ln_inv_evidence:.4f}\")\n",
    "print(f\"err_ln_inv_evidence: {err_ln_inv_evidence}\")\n",
    "\n",
    "# Convert to log-evidence (ln(Z) = -ln(1/Z)) and extract first error\n",
    "harmonic_logZ = -ln_inv_evidence\n",
    "err_ln_evidence = err_ln_inv_evidence[0] if not (np.isinf(err_ln_inv_evidence[0]) or np.isnan(err_ln_inv_evidence[0])) else 1.0  # Default to 1.0 if inf/nan\n",
    "\n",
    "\n",
    "# Print the computed log-evidence and its error in the requested format\n",
    "print(f\"Harmonic Log Evidence (RealNVPModel): {harmonic_logZ:.4f} +/- {err_ln_evidence:.4f}\")\n",
    "\n",
    "# Parse /data-synthetic/stats.dat to get Global Log-Evidence from synthetic\n",
    "with open('./mn_output/F24s-stats.dat', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if 'Nested Sampling Global Log-Evidence' in line:\n",
    "            # Extract the number after ':'\n",
    "            mn_logZ = float(line.split(':')[1].split('+/-')[0].strip())\n",
    "            err_mn_logZ = float(line.split(':')[1].split('+/-')[1].strip())\n",
    "            break\n",
    "\n",
    "print(f\"PyMultiNest log evidence (ln(Z)): {mn_logZ:.4f} +/- {err_mn_logZ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef136486",
   "metadata": {},
   "source": [
    "**Case**: `chains.add_chains_3d(samples, ln_posterior=log_weights)`\n",
    "```\n",
    "ln_inv_evidence: -54.8624\n",
    "err_ln_inv_evidence: (-0.004039131973917959, 0.004022882996576763)\n",
    "Harmonic Log Evidence (RealNVPModel): 54.8624 +/- -0.0040\n",
    "PyMultiNest log evidence (ln(Z)): -10.8605 +/- 0.0583\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7cad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import harmonic as hm\n",
    "\n",
    "# Section 1: Load randomscan\n",
    "# -------------------------------\n",
    "# Load the data file containing posterior weights, log-likelihoods, and parameters\n",
    "# Column 1: posterior weights\n",
    "# Column 2: log-likelihoods\n",
    "# Columns 3-9: parameters ['m0', 'm12', 'A0', 'tanb', 'mh', 'gminus2', 'omega']\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the data from the specified file and check for numerical issues.\"\"\"\n",
    "    data = np.loadtxt(file_path)\n",
    "    weights = data[:, 0]  # Posterior weights\n",
    "    log_likelihoods = data[:, 1]  # Log-likelihoods (already in logL form)\n",
    "    samples = data[:, 2:]  # Parameters (7 dimensions)\n",
    "    \n",
    "    # Check for inf/nan in log-likelihoods and samples\n",
    "    if np.any(np.isinf(log_likelihoods)) or np.any(np.isnan(log_likelihoods)):\n",
    "        print(\"Warning: log_likelihoods contain inf/nan values\")\n",
    "    if np.any(np.isinf(samples)) or np.any(np.isnan(samples)):\n",
    "        print(\"Warning: samples contain inf/nan values\")\n",
    "    \n",
    "    # Clip log-likelihoods to prevent extreme values\n",
    "    log_likelihoods = np.clip(log_likelihoods, -1e10, 1e10)\n",
    "    \n",
    "    # Print data statistics for debugging\n",
    "    print(f\"Number of samples: {len(samples)}\")\n",
    "    print(f\"Log-likelihood range: {log_likelihoods.min():.4f} to {log_likelihoods.max():.4f}\")\n",
    "    \n",
    "    return weights, log_likelihoods, samples\n",
    "\n",
    "file_path = './randomscan_output/F24s_random_withlogL.txt'\n",
    "weights, log_likelihoods, samples = load_data(file_path)\n",
    "\n",
    "# Normalize and threshold log-likelihoods\n",
    "log_likelihoods -= np.max(log_likelihoods)  # Shift max to 0\n",
    "logL_threshold = -25  # Cutoff to reduce dynamic range\n",
    "valid = log_likelihoods > logL_threshold\n",
    "samples = samples[valid]\n",
    "log_likelihoods = log_likelihoods[valid]\n",
    "\n",
    "# Reshape to (n_chains, n_samples_per_chain, n_dim)\n",
    "n_samples_total = samples.shape[0]\n",
    "n_chains = 4\n",
    "n_samples_per_chain = n_samples_total // n_chains\n",
    "\n",
    "samples = samples[:n_chains * n_samples_per_chain]\n",
    "log_likelihoods = log_likelihoods[:n_chains * n_samples_per_chain]\n",
    "\n",
    "samples = samples.reshape(n_chains, n_samples_per_chain, n_dim)\n",
    "log_likelihoods = log_likelihoods.reshape(n_chains, n_samples_per_chain)\n",
    "\n",
    "chains = hm.Chains(ndim=n_dim)\n",
    "chains.add_chains_3d(samples, ln_posterior=log_likelihoods)\n",
    "\n",
    "chains_train, chains_infer = hm.utils.split_data(chains, training_proportion=0.7)\n",
    "\n",
    "model = hm.model.RQSplineModel(\n",
    "    ndim_in=n_dim,\n",
    "    n_layers=4,                 # Reduced from default 8 for stability\n",
    "    n_bins=8,                   # Default, kept for flexibility\n",
    "    hidden_size=[64, 64],       # Default, kept for capacity\n",
    "    spline_range=(-5.0, 5.0),   # Narrowed to constrain transformations\n",
    "    standardize=True,           # Standardize data\n",
    "    learning_rate=0.001,        # Kept for stable convergence\n",
    "    momentum=0.9,               # Default for Adam optimizer\n",
    "    temperature=0.5             # Kept to tighten distribution\n",
    ")\n",
    "\n",
    "model.fit(chains_train.samples)\n",
    "\n",
    "ev = hm.Evidence(chains_infer.nchains, model)\n",
    "ev.add_chains(chains_infer)\n",
    "ln_inv_evidence = ev.ln_evidence_inv\n",
    "\n",
    "\n",
    "# Clip ln_inv_evidence to prevent inf\n",
    "if np.isinf(ln_inv_evidence) or np.isnan(ln_inv_evidence):\n",
    "    print(\"Warning: ln_inv_evidence is inf or nan, clipping to large value\")\n",
    "    ln_inv_evidence = np.clip(ln_inv_evidence, -1e10, 1e10)\n",
    "\n",
    "# Compute errors and handle inf/nan\n",
    "err_ln_inv_evidence = ev.compute_ln_inv_evidence_errors()\n",
    "print(f\"ln_inv_evidence: {ln_inv_evidence:.4f}\")\n",
    "print(f\"err_ln_inv_evidence: {err_ln_inv_evidence}\")\n",
    "\n",
    "# Convert to log-evidence (ln(Z) = -ln(1/Z)) and extract first error\n",
    "harmonic_logZ = -ln_inv_evidence\n",
    "err_ln_evidence = err_ln_inv_evidence[0] if not (np.isinf(err_ln_inv_evidence[0]) or np.isnan(err_ln_inv_evidence[0])) else 1.0  # Default to 1.0 if inf/nan\n",
    "\n",
    "\n",
    "# Print the computed log-evidence and its error in the requested format\n",
    "print(f\"Harmonic Log Evidence (RQSplineModel): {harmonic_logZ:.4f} +/- {err_ln_evidence:.4f}\")\n",
    "\n",
    "# Parse /data-synthetic/stats.dat to get Global Log-Evidence from synthetic\n",
    "print(f\"PyMultiNest log evidence (ln(Z)): {mn_logZ:.4f} +/- {err_mn_logZ:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678b117",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Why does the Harmonic evidence estimator require different inputs for `ln_posterior` when the prior is uniform, depending on whether the sampling is performed from the posterior distribution (e.g., via PyMultiNest) or from a uniform scan over the prior volume?\n",
    "\n",
    "In other words, if the prior is uniform in both cases, why must I pass `log_weights` (posterior weights) in the posterior-sampled case, but `log_likelihoods` in the prior-sampled case, to obtain consistent evidence estimates with Harmonic?\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of the answer\n",
    "\n",
    "The key difference lies in the **sampling distribution of the data**:\n",
    "\n",
    "- When samples are drawn **from the posterior** (Case A), the provided `log_weights` already encode the posterior density proportional to ℒ(θ) × π(θ). Thus, passing `ln_posterior = log_weights` correctly informs Harmonic about the distribution of samples and yields accurate evidence estimates.\n",
    "\n",
    "- When samples are drawn **from the prior** uniformly (Case B), all posterior weights are equal (e.g., 1), which does not represent the posterior distribution. In this case, you must pass `ln_posterior = log_likelihoods` so that Harmonic knows the posterior is proportional to the likelihood over the uniform prior samples. This allows Harmonic to correctly compute the evidence by integrating the likelihood over the prior volume.\n",
    "\n",
    "Therefore, even with a uniform prior in both cases, the difference in sampling distribution — posterior vs prior — dictates whether `ln_posterior` should be set to posterior weights or likelihood values for the evidence estimation to be consistent and accurate.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
